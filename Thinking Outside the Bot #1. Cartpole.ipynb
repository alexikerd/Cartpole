{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "Good afternoon.  In my free time I enjoy experimenting with AI.  Not just solving a problem but playing around with models and parameters to get a feel for what's really going on.  One of the topics I find most interesting is reinforcement learning (RL).  Instead of fitting a model to a dataset, RL forces us to be creative in creating an agent that experiences the data in the form of the game state and labels in the form of rewards.  Therefore, not only do we have to be careful in how we design the model, but we need to consider how we construct the game, inputs, and reward system.  However, despite the freedom inherent in reinforcement learning, there exists an underlying structure to each problem that the model is trying to learn.  Hopefully, during the course of this notebook I'll successfully illustrate both the structure behind RL as well as the methodology behind reinforcement learning as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/sample_random.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole\n",
    "\n",
    "In order to best discuss the core concepts behind RL, I've decided to use the simplest use case known as the cartpole problem.  It consists of a pole standing on top of a cart.  The player can accelerate the cart right or left, moving the pivot point in an attempt to prevent the pole from falling.  The model is fed four different inputs: the x position of the cart ($x$), the x velocity of the cart ($\\dot x$), the angular position of the pole ($\\theta$), and the angular velocity of the pole ($\\dot \\theta$).\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/ObservationSpace.PNG?raw=true)\n",
    "\n",
    "At the beginning of the game, each of the four values are randomly initialized between -0.05 and 0.05 so that each initial state is slightly different but close to zero.  The game ends when either the absolute value of the x position exceeds 2.4, the pole angle exceeds 41.8$^{\\circ}$, or the game lasts for 500 frames.  The limitation on the cart is a lot more lenient than the pole, which means any agent training to balance the pole will prioritize the pole's position over the cart's.  The agent has two options; accelerate the cart left or right.\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/ActionSpace.PNG?raw=true)\n",
    "\n",
    "Note that the agent is not allowed to keep the current speed.  The agent is also not allowed to directly adjust the pole speed.  Instead it must move the cart and use the resultant torque to keep the pole standing up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics\n",
    "\n",
    "Before I get into the models I've used, let's talk about the environment and the physics behind it.  The cartpole is an example of an equilibrium problem, where we are trying to maintain a certain position (pole being straight up).  Furthermore, this is a type of equilibrium that is known as unstable equilibrium where the slightest deviation from the target position leads to further deviation without intervention.  As the horizonal distance limit is much larger than the angle limit, we can ignore it right now in order to simplify our analysis of the environment.  The two relevant equations are \n",
    "\n",
    "$$\\dfrac{\\delta \\theta}{\\delta t} \\approx \\dot \\theta$$\n",
    "$(1)$\n",
    "\n",
    "$$\\dfrac{\\delta \\dot \\theta}{\\delta t} \\approx \\sin \\theta$$\n",
    "$(2)$\n",
    "\n",
    "Anyone who has studied physics will know that $\\sin \\theta \\approx \\theta$ for small angles as this is a very popular trick in derivations.  Since the purpose of the cartpole to is maintain small angular displacement from standing straight up, we can safely assume angles will be \"small\" enough.  Therefore, it seems to me that we can solve an equation balancing pole with a linear model as both $\\theta$ and $\\dot \\theta$ rely on the other in a linear relationship.  Keep in mind that we are briefly ignoring the x constraint.  To further describe the equilibrium system in this way, here are two graphs.\n",
    "\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/PotentialEnergyGraph.png?raw=true)|![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/PhaseSpaceDiagram.png?raw=true)\n",
    "-|-\n",
    "(A)|(B)\n",
    "\n",
    "Figure A demonstrates the instability of the potential energy.  This is a good initial assessment as every system tends to move towards lowest potential energy.  Figure B is known as a phase space diagram, where we graph the rate of change in terms of both position and velocity.  As we can see, the further the angle is from zero, the greater the acceleration (rate of change of velocity).  The greater the angular velocity, the greater the rate of change of angle (obviously, but what matters it that the rate of change is pointed away from the center or equilibrium point).  While describing the same phenomena, the phase space diagram does a much better job because we can see that the velocity is much more dangerous to equilibrium than the angle (this is because the game ends at smaller angles so gravity does not get a chance to really affect the pole).  Another thing to note is that the velocity values will seem higher than possible.  This is because the game runs at 50 fps so that any model has the ability to update the speed every 0.02 seconds.  The dynamics of the system can be explored with the Lagrangian and the Euler-Lagrange equation.\n",
    "\n",
    "$$\\begin{equation}L = T - V\\end{equation}$$\n",
    "$(3)$\n",
    "\n",
    "\n",
    "$$\\dfrac{d}{dt}\\dfrac{\\delta L}{\\delta\\dot q} = \\dfrac{\\delta L}{\\delta q}$$\n",
    "$(4)$\n",
    "\n",
    "Eq (3) sets the lagrangian (L) equal to the total kinetic energy of the system (T) minus the total potential energy of the system (V).  As there is no friction, the lagrangian will be a constant.  Deriving lagrangian mechanics is a little out of the scope for this report, but I'll leave links at the end for those interested.  The first link is a mathematical proof that the Euler-Lagrange equation solves for the shortest path between two points and the second provides an explanation that eq (4) solves for the shortest path through the energy of the system.  For now, I'll keep with the assumption that solving eq (4) properly will solve the practical dynamics of the cartpole (and any similar RL environments).  The EL equation states that the derivative of the lagrange with respect to the spatial coordinates (q) must be equal to the derivative with respect to time of the derivative with respect to the rate of change of the spatial coordinates of the lagrange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "Before we build a model, the first step is to establish simple heuristics in order to ground our expectations.  I played the cartpole myself and got an average around 40.  I could have practised and gotten better at the game but I'd like to avoid cartpole tunnel syndrome.  The Suicide heuristic is one that attempts to accelerate the pole towards the ground, ending on an average of 9 frames.  This is the worst a model can perform and is what happens when a model only predicts one direction.\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/suicide_model_sample.gif?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "Playing randomly averages around 22 frames.  This is the best baseline, as any model that outperforms this will have at least learned something about the environment.  While playing randomly extends the duration of the game, we can see in the phase space diagram that the heuristic is incapable of reacting to the environment, allowing the instability to increase and push it away from the center.\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/random_model_sample.gif?raw=true)\n",
    "\n",
    "\n",
    "Controlling the angle, or accelerating to the right when the angle is positive and to the left when the angle is negative, yields an average of 41 frames.  This heuristic is able to adapt to the environment, but quickly gains energy and momentum that spirals the pole away from the center.\n",
    "\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/controlled_angle_sample.gif?raw=true)\n",
    "\n",
    "Because the angular velocity is more volatile than the angle itself, it's no surprise that controlling the velocity yields an average of 200 frames.  On an initial glance, it may seem that the heuristic works perfectly well and that the only issue is the $x$ constraint.  However, the phase space diagram exposes the fact that the angle is slowly increasing as well.  This is because the greater the angle, the more the pole tends to increase speed in that direction.  Merely controlling the speed will then cause the pole to slip as the average speed will be slightly in favor of increasing towards the pole's fall.\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/controlled_velocity_sample.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Now, before we can begin building models, I'd like to define a few terms and explain the concept behind reinforcement learning.  Reinforcement learning (RL) has three main components:\n",
    "\n",
    "1. <b>Environment</b>.  The environment is the ruleset, underlying physics, reward system, and anything else that is not the agent.  \n",
    "2. <b>Actions</b>.  Actions are a list of possible behaviours that the agent can take.  While the list of available actions are determined by the state\n",
    "3. <b>Noniterable Dataloader</b>.  Another issue was that I generated image augmentation during dataset creation, outside of the dataloader.  Even though each image was turned into 15 images, they were the same 15 images regardless of epoch.  This is a somewhat minor detail, but I made adjustments to the ImageDataset class object that performed the rotation and cropping of the image with each iteration. Therefore, each epoch of the original is equal to ~15 epochs of the new dataset.  However, I was able to train for 400 epochs for the new model and only ran for 10 epochs on the old.\n",
    "4. <b>Image Sizes</b>.  Another big issue has to do with the size of the image.  Both models perform better when given an image the same dimensions as the data they were trained on.  In theory, I could have randomly resized the training data in order to compensate for this weakness, but this did not occur to me until I started writing this report and comparing the two models.  Regardless, another big impact that size had was the capacity of the GPU.  I had to make cuts to the neural networks and only run on a batch size of 1 in order to be able to fit the 600X800 images.  However, when I converted all images to 100X100 I was not only able to maintain the integrity of the UNet structure and run batches of 16, but I was able to train both the TNet and MNet together.  I named this new network the GluNet (GNet).  This is a much better strategy as it allows for both networks to learn from each other and work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class MarkovAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.memory = []\n",
    "        self.Qs = {}\n",
    "        self.Ns = {}\n",
    "        self.done = False\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1\n",
    "        self.decay = 0.99\n",
    "        \n",
    "        self.session_score = 0\n",
    "        self.max_distance = 0\n",
    "        \n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.done = False\n",
    "        self.memory = []\n",
    "        \n",
    "    def play_session(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        self.session_score = 0\n",
    "        self.max_distance = 0\n",
    "        \n",
    "        while not self.done:\n",
    "            \n",
    "            s,action = self.take_action(observation)\n",
    "            observation, reward, self.done, info = self.env.step(action)\n",
    "            self.session_score += 1            \n",
    "            self.memory.append([s,-100 if self.done and self.session_score<500 else reward,action])\n",
    "            \n",
    "            distance = observation[0]\n",
    "            \n",
    "            if abs(distance)>self.max_distance:\n",
    "                self.max_distance = abs(distance)\n",
    "            \n",
    "        past_reward = 0\n",
    "        \n",
    "        for s, reward, action in reversed(self.memory):\n",
    "            \n",
    "            self.Qs[s][action] = (self.Qs[s][action]*self.Ns[s][action] + (reward + self.gamma*past_reward))/(self.Ns[s][action]+1)\n",
    "            self.Ns[s][action] += 1\n",
    "            past_reward = (reward + self.gamma*past_reward)\n",
    "            \n",
    "        self.epsilon = max(self.decay*self.epsilon,0.05)\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def take_action(self,observation):\n",
    "        \n",
    "        s = np.asarray([round(observation[i],1+int(i/2)) for i in range(len(observation))]).tostring()\n",
    "        if s not in self.Ns:\n",
    "            \n",
    "            self.Ns[s] = [5,5]\n",
    "            self.Qs[s] = [100,100]\n",
    "            \n",
    "        q_list = self.Qs[s]\n",
    "        \n",
    "        if np.random.uniform(0,1)>self.epsilon:\n",
    "            \n",
    "            action = np.argmax(q_list)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            action = self.env.action_space.sample()\n",
    "            \n",
    "        return s,action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/training_markov.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/markov_decisionboundary.png?raw=true)|![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/svm_decisionboundary.png?raw=true)\n",
    "-|-\n",
    "A|B\n",
    "\n",
    "\n",
    "I increased limit to 1000 just to test the model and wowza\n",
    "\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/svm_model_sample.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class LinearModel():\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.matrix = np.random.rand(self.input_dim,self.output_dim)\n",
    "        self.bias = np.random.uniform(-1,1)\n",
    "        \n",
    "    def predict(self,observation):\n",
    "        \n",
    "        return np.sum([self.matrix[i]*observation[i] for i in range(self.input_dim)],axis=0) + self.bias\n",
    "    \n",
    "    def update(self,instance,lr):\n",
    "        \n",
    "        state = instance[:4]\n",
    "        action = instance[4]\n",
    "        reward = instance[5]\n",
    "        \n",
    "        forward_feed = [self.matrix[i][action]*state[i] for i in range(self.input_dim)]\n",
    "        error = reward - (np.sum(forward_feed) + self.bias)\n",
    "        adjustment = error*lr\n",
    "        \n",
    "        self.bias += adjustment\n",
    "        self.matrix[:,action] += np.asarray(state)*adjustment\n",
    "\n",
    "\n",
    "\n",
    "class CartpoleAgent():\n",
    "    \n",
    "    def __init__(self,minmem,maxmem):\n",
    "        \n",
    "        self.minmem = minmem\n",
    "        self.maxmem = maxmem\n",
    "        self.memory = deque([], maxlen=self.maxmem)\n",
    "        self.epsilon = 1\n",
    "        self.decay = 0.995\n",
    "        self.gamma = 0.95\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.model = LinearModel(4,2)\n",
    "\n",
    "        self.scores = []\n",
    "        self.means = []\n",
    "        self.losses = []\n",
    "\n",
    "    def predict(self,observation):\n",
    "        \n",
    "        return self.model.predict(observation)\n",
    "    \n",
    "    def remember(self,batch_size):\n",
    "        \n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def train(self,epochs,num_games,batch_size):\n",
    "        \n",
    "        e = 0\n",
    "        \n",
    "        while e<epochs:\n",
    "            \n",
    "            for _ in range(num_games):\n",
    "            \n",
    "                done = False\n",
    "                observation = self.env.reset()\n",
    "                trainexamples = []\n",
    "                session_score = 0\n",
    "\n",
    "                while not done:\n",
    "\n",
    "                    if np.random.uniform(0,1)>self.epsilon:\n",
    "                        action = np.argmax(self.model.predict(observation))\n",
    "                    else:                    \n",
    "                        action = self.env.action_space.sample()\n",
    "\n",
    "                    session_score += 1\n",
    "                    next_state, reward, done, info = self.env.step(action)\n",
    "                    trainexamples.append([observation,action,-100 if done and session_score<500 else 1,next_state])\n",
    "                    observation = next_state\n",
    "\n",
    "                self.scores.append(session_score)\n",
    "\n",
    "                current_reward = 0\n",
    "                for example in reversed(trainexamples):\n",
    "                    current_reward = example[2] + self.gamma*current_reward\n",
    "                    self.memory.append([example[0][0],example[0][1],example[0][2],example[0][3],example[1],current_reward])                 \n",
    "                     \n",
    "                    \n",
    "            mean = np.mean(self.scores[-num_games:])\n",
    "            print(f'Epoch {e+1} had a mean score of {mean} over {num_games} games.')\n",
    "            self.means.append(mean)\n",
    "            \n",
    "                \n",
    "            \n",
    "            self.epsilon = max(0.05,self.decay*self.epsilon)\n",
    "            \n",
    "            if len(self.memory)>self.minmem:\n",
    "                \n",
    "                batch = self.remember(batch_size)\n",
    "                for b in batch:\n",
    "                    self.model.update(b,self.lr)\n",
    "                    \n",
    "                e += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/LinearModelTrainingwoAgency.png?raw=true)|![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/LinearModelTrainingwAgency.png?raw=true)\n",
    "-|-\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/linear_model_random.gif?raw=true)|![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/linear_model_best.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "\n",
    "1. Derivation of Euler-Lagrange equation https://farside.ph.utexas.edu/teaching/336L/Fluid/node266.html\n",
    "2. Explanation of lagrangian mechanics http://www.physicsinsights.org/lagrange_1.html\n",
    "3. Solution to Cartpole problem using physics https://danielpiedrahita.wordpress.com/portfolio/cart-pole-control/\n",
    "4. How to handle P(s) and Q(s) with state as input https://web.stanford.edu/~surag/posts/alphazero.html\n",
    "5. Good analysis of simple models to solve cartpole http://kvfrans.com/simple-algoritms-for-solving-cartpole/\n",
    "6. Stackoverflow covering linear model to solve cartpole https://stats.stackexchange.com/questions/250531/understanding-oscillating-behaviour-when-using-q-learning-on-cart-pole-problem\n",
    "7. Discussion on overfitting in Deep RL https://arxiv.org/pdf/1804.06893.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
