{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "Good afternoon.  In my free time I enjoy experimenting with AI.  Not just solving a problem but playing around with models and parameters to get a feel for what's really going on.  One of the topics I find most interesting is reinforcement learning (RL).  Instead of fitting a model to a dataset, RL forces us to be creative in creating an agent that experiences the data in the form of the game state and labels in the form of rewards.  Therefore, not only do we have to be careful in how we design the model, but we need to consider how we construct the game, inputs, and reward system.  However, despite the freedom inherent in reinforcement learning, there exists an underlying structure to each problem that the model is trying to learn.  Hopefully, during the course of this notebook I'll successfully illustrate both the structure behind RL as well as the methodology behind reinforcement learning as a whole.\n",
    "\n",
    "First, let me define some necessary terms used frequently in reinforcement learning.\n",
    "\n",
    "1. <b>Environment</b>.  The environment is anything that pertains to the game being played.  This includes the rules for the game, the current state of the game, and the players participating in the game.\n",
    "1. <b>Observation</b>.  The observation is a representation of all the relevant information about the environment.  The mathematical name for the group of all possible states for a given environment is its state space.\n",
    "1. <b>Agent</b>.  An agent, put simply, is any model that receives input (usually in the form of a game state fed from the environment) and gives an output (usually in the form of a policy gradient ${\\pi}$).  The agent is also able to interact with the environment in this way iteratively, tuning its model to produce better policy gradients in order to optimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/sample_random.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole\n",
    "\n",
    "In order to best discuss the core concepts behind RL, I've decided to use the simplest use case known as the cartpole problem.  It consists of a pole standing on top of a cart.  The player can accelerate the cart right or left, moving the pivot point in an attempt to prevent the pole from falling.  The model is fed four different inputs: the x position of the cart, the x velocity of the cart, the angular position of the pole, and the angular velocity of the pole.\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/ObservationSpace.PNG?raw=true)\n",
    "\n",
    "At the beginning of the game, each of the four values are randomly initialized between -0.05 and 0.05 so that each initial state is slightly different but close to zero.  The game ends when either the absolute value of the x position exceeds 2.4, the pole angle exceeds 41.8$^{\\circ}$, or the game lasts for 500 frames.  The limitation on the cart is a lot more lenient than the pole, which means any agent training to balance the pole will prioritize the pole's position over the cart's.  The agent has two options; accelerate the cart left or right.\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/ActionSpace.PNG?raw=true)\n",
    "\n",
    "Note that the agent is not allowed to keep the current speed.  The agent is also not allowed to directly adjust the pole speed.  Instead it must move the cart and use the resultant torque to keep the pole standing up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics\n",
    "\n",
    "Before I get into the models I've used, let's talk about the environment and the physics behind it.  The cartpole is an example of an equilibrium problem, where we are trying to maintain a certain position (pole being straight up).  Furthermore, this is a type of equilibrium that is known as unstable equilibrium where the slightest deviation from the target position leads to further deviation without intervention.  As the horizonal distance limit is much larger than the angle limit, we can ignore it right now in order to simplify our analysis of the environment.\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/PotentialEnergyGraph.png?raw=true)|![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/PhaseSpaceDiagram.png?raw=true)\n",
    "-|-\n",
    "(A)|(B)\n",
    "\n",
    "Figure A demonstrates the instability of the potential energy.  This is a good initial assessment as every system tends to move towards lowest potential energy.  Figure B is known as a phase space diagram, where we graph the rate of change in terms of both position and velocity.  As we can see, the further the angle is from zero, the greater the acceleration (rate of change of velocity).  The greater the angular velocity, the greater the rate of change of angle (obviously, but what matters it that the rate of change is pointed away from the center or equilibrium point).  While describing the same phenomena, the phase space diagram does a much better job because we can see that the velocity is much more dangerous to equilibrium than the angle (this is because the game ends at smaller angles so gravity does not get a chance to really affect the pole).  Another thing to note is that the velocity values will seem higher than possible.  This is because the game runs at 50 fps so that any model has the ability to update the speed every 0.02 seconds.  The dynamics of the system can be explored with the Lagrangian and the Euler-Lagrange equation.\n",
    "\n",
    "$$\\begin{equation}L = T - V\\end{equation}$$\n",
    "$(3)$\n",
    "\n",
    "\n",
    "$$\\dfrac{d}{dt}\\dfrac{\\delta L}{\\delta\\dot q} = \\dfrac{\\delta L}{\\delta q}$$\n",
    "$(4)$\n",
    "\n",
    "Eq (3) sets the lagrangian (L) equal to the total kinetic energy of the system (T) minus the total potential energy of the system (V).  As there is no friction, the lagrangian will be a constant.  Deriving lagrangian mechanics is a little out of the scope for this report, but I'll leave links at the end for those interested.  The first link is a mathematical proof that the Euler-Lagrange equation solves for the shortest path between two points and the second provides an explanation that eq (4) solves for the shortest path through the energy of the system.  For now, I'll keep with the assumption that solving eq (4) properly will solve the practical dynamics of the cartpole (and any similar RL environments).  The EL equation states that the derivative of the lagrange with respect to the spatial coordinates (q) must be equal to the derivative with respect to time of the derivative with respect to the rate of change of the spatial coordinates of the lagrange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/suicide_model_sample.gif?raw=true)\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/random_model_sample.gif?raw=true)\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/controlled_angle_sample.gif?raw=true)\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/controlled_velocity_sample.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MarkovAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.memory = []\n",
    "        self.Qs = {}\n",
    "        self.Ns = {}\n",
    "        self.done = False\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1\n",
    "        self.decay = 0.99\n",
    "        \n",
    "        self.session_score = 0\n",
    "        self.max_distance = 0\n",
    "        \n",
    "        self.env = gym.make(\"CartPole-v3\")\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.done = False\n",
    "        self.memory = []\n",
    "        \n",
    "    def play_session(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        self.session_score = 0\n",
    "        self.max_distance = 0\n",
    "        \n",
    "        while not self.done:\n",
    "            \n",
    "            s,action = self.take_action(observation)\n",
    "            observation, reward, self.done, info = self.env.step(action)\n",
    "            self.session_score += 1            \n",
    "            self.memory.append([s,-100 if self.done and self.session_score<2000 else reward,action])\n",
    "            \n",
    "            distance = observation[0]\n",
    "            \n",
    "            if abs(distance)>self.max_distance:\n",
    "                self.max_distance = abs(distance)\n",
    "            \n",
    "        past_reward = 0\n",
    "        \n",
    "        for s, reward, action in reversed(self.memory):\n",
    "            \n",
    "            self.Qs[s][action] = (self.Qs[s][action]*self.Ns[s][action] + (reward + self.gamma*past_reward))/(self.Ns[s][action]+1)\n",
    "            self.Ns[s][action] += 1\n",
    "            past_reward = (reward + self.gamma*past_reward)\n",
    "            \n",
    "        self.epsilon = max(self.decay*self.epsilon,0.05)\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def take_action(self,observation):\n",
    "        \n",
    "        s = np.asarray([round(observation[i],1+int(i/2)) for i in range(len(observation))]).tostring()\n",
    "        if s not in self.Ns:\n",
    "            \n",
    "            self.Ns[s] = [5,5]\n",
    "            self.Qs[s] = [100,100]\n",
    "            \n",
    "        q_list = self.Qs[s]\n",
    "        \n",
    "        if np.random.uniform(0,1)>self.epsilon:\n",
    "            \n",
    "            action = np.argmax(q_list)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            action = self.env.action_space.sample()\n",
    "            \n",
    "        return s,action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/training_markov.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/markov_decisionboundary.png?raw=true)|![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/svm_decisionboundary.png?raw=true)\n",
    "-|-\n",
    "A|B\n",
    "\n",
    "\n",
    "I increased limit to 1000 just to test the model and wowza\n",
    "\n",
    "\n",
    "![Original Image](https://github.com/alexikerd/Cartpole/blob/main/graphics/svm_model_sample.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "\n",
    "1. Derivation of Euler-Lagrange equation https://farside.ph.utexas.edu/teaching/336L/Fluid/node266.html\n",
    "2. Explanation of lagrangian mechanics http://www.physicsinsights.org/lagrange_1.html\n",
    "3. Solution to Cartpole problem using physics https://danielpiedrahita.wordpress.com/portfolio/cart-pole-control/\n",
    "4. How to handle P(s) and Q(s) with state as input https://web.stanford.edu/~surag/posts/alphazero.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
